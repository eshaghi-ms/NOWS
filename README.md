# NOWS: Neural Operator Warm Starts

This repository accompanies the paper:

**NOWS: Neural Operator Warm Starts for Accelerating Iterative Solvers**

---

## Overview

**Neural Operator Warm Starts (NOWS)** is a hybrid framework that accelerates classical numerical PDE solvers â€” such as Conjugate Gradient (CG) and GMRES â€” by using *neural operators* (e.g., FNO, DeepONet, VINO) to generate high-quality initial guesses.  
This reduces solver iterations and overall runtime while keeping the solverâ€™s **stability**, **robustness**, and **convergence guarantees** fully intact.

NOWS is **solver-agnostic** and **discretization-agnostic**, integrating seamlessly with:
- Finite Element Method (FEM)
- Finite Difference Method (FDM)
- Isogeometric Analysis (IGA)
- Finite Volume Method (FVM)

Across diverse PDEs (Poisson, Darcy, Burgers, elasticity, Navierâ€“Stokes), NOWS consistently achieves **25â€“90% runtime reduction**, demonstrating strong generalization across geometries, resolutions, and physical systems.

---
## NOWS framework (high-level)

<p align="center">
  <img src="docs/figs/flowchart.png" width="700">
</p>

*Workflow concept:*  
Neural operator â†’ produces an initial guess â†’ classical solver only needs to do the *fine* iterations â†’ same accuracy, much lower cost.

<p align="center">
  <img src="docs/figs/dynamic.png" width="700">
</p>

---

## Repository status

The **code and data will be made publicly available after the journal review process is complete.**

ğŸ‘‰ **Please â­ star this repository** to get notified automatically when the public release is published.

---

## Abstract (summary)

Partial differential equations (PDEs) are the foundation of computational science and engineering, yet high-fidelity simulations remain computationally demanding.  
NOWS introduces a hybrid paradigm that leverages neural operator predictions to warm-start traditional iterative solvers.  
The neural operator rapidly provides an accurate initial guess, and the classical solver completes convergence.  
This hybridization achieves substantial speed-ups (up to 90%), retains full accuracy, and is compatible with any existing solver infrastructure.  
By combining the rapid inference of learned operators with the rigor of numerical solvers, NOWS offers a practical and trustworthy path toward accelerating high-fidelity PDE simulations.

---

## Key features

- ğŸš€ **Acceleration:** Reduces iteration count and runtime by up to 90%.
- ğŸ§  **Physics-consistent learning:** Can use data-driven, physics-informed, or hybrid training.
- âš™ï¸ **Solver-agnostic:** Works with CG, GMRES, and other Krylov or multigrid methods.
- ğŸ§© **Discretization-independent:** Supports FEM, FDM, IGA, and FVM backends.
- ğŸ—ºï¸ **Generalization:** Robust to mesh refinement, geometry variation, and PDE type.
- ğŸ§® **No modification needed:** Integrates directly with existing solvers.

---

## Project structure (will be available upon release)

NOWS/

â”‚

â”œâ”€â”€ nows/                  # Core implementation

â”‚   â”œâ”€â”€ models/            # Neural operator architectures (FNO, VINO, etc.)

â”‚   â”œâ”€â”€ solvers/           # Interfaces to iterative solvers (CG, GMRES, ...)

â”‚   â”œâ”€â”€ utils/             # Helper functions and data utilities

â”‚   â””â”€â”€ training/          # Physics-informed and data-driven training scripts

â”‚

â”œâ”€â”€ experiments/           # Reproducible benchmark setups

â”‚   â”œâ”€â”€ poisson/

â”‚   â”œâ”€â”€ darcy/

â”‚   â”œâ”€â”€ elasticity/

â”‚   â”œâ”€â”€ burgers/

â”‚   â””â”€â”€ smoke_plume/

â”‚

â”œâ”€â”€ data/                  # Sample or synthetic datasets

â”‚

â”œâ”€â”€ notebooks/             # Interactive demos

â”‚

â”œâ”€â”€ results/               # Plots, figures, and numerical comparisons

â”‚

â”œâ”€â”€ LICENSE

â”œâ”€â”€ README.md

â””â”€â”€ requirements.txt


## Contact

For questions or collaboration inquiries:

- **Mohammad Sadegh Eshaghi** â€” eshaghi.khanghah@iop.uni-hannover.de  
- **Cosmin Anitescu** â€” cosmin.anitescu@uni-weimar.de  

---

## Acknowledgments

The authors acknowledge the support of:

- The **German Academic Exchange Service (DAAD)**
- The **Compute Servers of TU Ilmenau** for providing computational resources.
- 
## Paper citation

If you use or refer to this work, please cite:

```bibtex
@article{Eshaghi2025NOWS,
  title={Neural Operator Warm Starts for Accelerating Iterative Solvers},
  author={Eshaghi, Mohammad Sadegh and Anitescu, Cosmin and Valizadeh, Navid and Wang, Yizheng and Zhuang, Xiaoying and Rabczuk, Timon},
  year={2025},
  journal={...}
}
